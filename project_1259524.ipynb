{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Automated Fact Checking For Climate Science Claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Hongda Zhu\n",
    "\n",
    "Student ID: 1259524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.16 (default, Mar  1 2023, 21:19:10) \n",
      "[Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "train_data = load_data(\"train-claims.json\")\n",
    "dev_data = load_data(\"dev-claims.json\")\n",
    "evidence_data = load_data(\"evidence.json\")\n",
    "\n",
    "# print(\"train_data: \", train_data)\n",
    "# print(\"dev_data: \", dev_data)\n",
    "# print(\"evidence_data: \", evidence_data)\n",
    "# for id, info in train_data.items():\n",
    "#     print(\"claim_id: \", id)\n",
    "#     print(\"claim_text: \", info['claim_text'])\n",
    "#     print(\"claim_label: \", info['claim_label'])\n",
    "#     print(\"evidences: \", info['evidences'])\n",
    "#     print()\n",
    "# print(\"dev_data: \", dev_data)\n",
    "# print(\"evidence_data: \", evidence_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Replace claims evidence id with actual evidence content in evidence data\n",
    "# Tokenize them and combine together as the preprocessed data\n",
    "def preprocess_data(claim_data, evidence_data, tokenizer):\n",
    "    preprocessed_data = {}\n",
    "    for claim_id, claim_info in claim_data.items():\n",
    "        claim_text = claim_info[\"claim_text\"]\n",
    "        evidence_ids = claim_info[\"evidences\"]\n",
    "        claim_label = claim_info[\"claim_label\"]\n",
    "\n",
    "        # Tokenize claim data\n",
    "        claim_tokens = tokenizer.tokenize(claim_text)\n",
    "\n",
    "        # Tokenize evidence data\n",
    "        evidence_tokens = []\n",
    "        for evidence_id in evidence_ids:\n",
    "            evidence_text = evidence_data[evidence_id]\n",
    "            tokens = tokenizer.tokenize(evidence_text)\n",
    "            evidence_tokens.append(tokens)\n",
    "\n",
    "        # Combine and store preprocessed data\n",
    "        preprocessed_data[claim_id] = {\n",
    "            \"claim_tokens\": claim_tokens,\n",
    "            \"evidence_tokens\": evidence_tokens,\n",
    "            \"claim_label\": claim_label,\n",
    "        }\n",
    "    return preprocessed_data\n",
    "\n",
    "# Preprocess the train and dev data\n",
    "preprocessed_train_data = preprocess_data(train_data, evidence_data, tokenizer)\n",
    "preprocessed_dev_data = preprocess_data(dev_data, evidence_data, tokenizer)\n",
    "\n",
    "# print(\"preprocessed_train_data: \", preprocessed_train_data)\n",
    "# print(\"preprocessed_dev_data: \", preprocessed_dev_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PyTorch Dataset and DataLoader for preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing samples from the train_dataset:\n",
      "Sample 1:\n",
      "  input_ids:  tensor([[  101,  2025,  2069,  2003,  2045,  2053,  4045,  3350,  2008,  2522,\n",
      "          2475,  2003,  1037,  8554, 13210,  3372,  1010,  3020,  2522,  2475,\n",
      "         14061,  2941,  2393, 20440,  2490,  2062,  3269,  1998,  4111,  2166,\n",
      "          1012,   102,  2012,  2200,  2152, 14061,  1006,  2531,  2335, 12483,\n",
      "          6693,  1010,  2030,  3618,  1007,  1010,  6351, 14384,  2064,  2022,\n",
      "         11704,  2000,  4111,  2166,  1010,  2061,  6274,  1996,  6693,  2000,\n",
      "          2184,  1010,  2199,  4903,  2213,  1006,  1015,  1003,  1007,  2030,\n",
      "          3020,  2005,  2195,  2847,  2097, 11027, 20739,  2015,  2107,  2004,\n",
      "          2317, 24019,  1998,  6804, 10210,  2229,  1999,  1037, 16635,  1012,\n",
      "           102]])\n",
      "  attention_mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "  labels:  tensor(3)\n",
      "\n",
      "Sample 2:\n",
      "  input_ids:  tensor([[  101,  3449, 20801,  5225,  2501, 26836,  1999,  3795,  7715,  9104,\n",
      "          4125,  2089,  2025,  2022,  2091,  2000,  2158,  1011,  2081, 11768,\n",
      "          1012,   102,  2096,  1520,  4785,  2689,  1521,  2064,  2022,  2349,\n",
      "          2000,  3019,  2749,  2030,  2529,  4023,  1010,  2045,  2003,  2085,\n",
      "          6937,  3350,  2000,  5769,  2008,  2529,  4023,  1516,  1998,  4919,\n",
      "          3445, 16635,  3806,  1006,  1043, 25619,  2015,  1007, 11768,  1516,\n",
      "          2003,  1037,  3145,  5387,  1999,  1996,  6393,  1998,  6698,  1997,\n",
      "          3795,  4860,  7457,  1012,   102]])\n",
      "  attention_mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])\n",
      "  labels:  tensor(1)\n",
      "\n",
      "Sample 3:\n",
      "  input_ids:  tensor([[  101,  1999,  3918,  1010, 22851,  2080,  7237,  2000,  1037,  4658,\n",
      "          4403,  1012,   102,  2045,  2003,  3350,  1997, 23163,  2015,  1999,\n",
      "          1996, 19283, 11508,  3012,  1006,  3574,  3431,  1999,  4658,  3302,\n",
      "          5380,  6431,  4010,  3302,  5380,  2306,  1996,  2555,  1007,  1997,\n",
      "          1996,  9808,  6895, 20382, 10066,  2105,  4849,  1010,  4006,  1010,\n",
      "          1998,  3355,  1025,  1996,  2197,  2048, 23163,  2015, 27601,  2007,\n",
      "          6918, 12363,  1999, 11840,  2537, 25228,  1999,  1996,  2167,  3534,\n",
      "          4153,  1012,   102]])\n",
      "  attention_mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])\n",
      "  labels:  tensor(0)\n",
      "\n",
      "Printing a batch from the train_dataloader:\n",
      "Batch 1:\n",
      "  input_ids:  tensor([[  101,  1996,  3623,  1999,  4053,  1999,  3522,  2086,  2003,  2349,\n",
      "          2000,  2313,  3930,  1999,  8211,  2752,  1998,  3532,  3224,  2968,\n",
      "          1012,   102,  1996,  2458,  1997,  5237,  9124,  1996,  2529,  2313,\n",
      "          2000,  4982,  2116,  2335,  3469,  2084,  2071,  2022,  8760,  2011,\n",
      "          5933,  1998,  7215,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2045,  2024,  2055,  6036,  1010,  2199,  5943,  2943,  5841,\n",
      "          1999,  1996,  2142,  2163,  1010,  2021,  2069,  1015,  1010,  6352,\n",
      "          1997,  2068,  2024,  1999,  4108,  1012,   102,  2004,  1997,  2355,\n",
      "          1010,  2062,  2084, 13539,  1010,  2199,  2111,  2499,  1999,  1996,\n",
      "          5943,  3068,  1998,  4724,  2163,  7333,  5658,  8316,  2075,  1010,\n",
      "          2073,  2943, 16548,  4149,  2067,  9987,  2373,  7013,  2011,  5943,\n",
      "         27448,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1006,  2384,  1007,  1010,  2073,  5871, 12456, 14428, 11129,\n",
      "          2511,  2008,  1996,  2812, 14983,  1997,  1996,  2972, 16128,  3256,\n",
      "          7123,  2018,  3445,  2012,  1016,  5282,  2566,  2095,  1516,  1037,\n",
      "          2561,  1997,  2471,  1016,  2519,  1516,  1999,  1996,  2340,  2086,\n",
      "          2857,  1011,  2494,  1012,  1524,   102,  2127,  2289,  1010,  3446,\n",
      "          1997,  9885,  1999,  3256,  7123,  4578,  1999,  4642,  2566,  2095,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1523,  2000,  7065, 17417,  2102,  1996,  4153,  1521,  1055,\n",
      "          5122, 12184,  8737,  6906, 22662,  2085,  1010,  2057,  2342,  2000,\n",
      "          5362, 24110, 27351,  2023,  2128,  1011,  1041, 26147, 12322,  8156,\n",
      "          1010,  2029,  2038,  2042, 17092,  2005,  2205,  2146,  1012,   102,\n",
      "          1996, 10250, 12322,  8156,  2001,  3322,  2589,  2006,  1996,  3978,\n",
      "          1997, 13589,  8358,  1999,  4860,  1998,  2009,  2001,  5071,  2008,\n",
      "          2023, 27601,  2000, 15850,  8358,  1006,  8183, 20395,  2140,  1998,\n",
      "         21442,  3669, 22879,  1010,  3118,  1007,  1012,   102],\n",
      "        [  101,  4286,  5175,  2627,  4785,  3431,   102,  2028,  1997,  1996,\n",
      "          2364,  8106,  2000,  1996, 14446,  2003,  4785,  2689,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  8044,  3073,  4997, 12247,  1012,   102, 13644,  4092,  1010,\n",
      "          2065,  8044,  1010,  2926,  2659,  8044,  1010,  3623,  1999,  1037,\n",
      "         16676,  4785,  1010,  1996, 28573, 11520,  3466,  5260,  2000,  1037,\n",
      "          4997, 12247,  1999,  4785,  3433,  2000,  3445, 16635, 15865,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  4785, 21796,  2036,  7868,  2008,  4774,  3011,  2003,  2025,\n",
      "          8790,   102,  2023,  2944,  2038,  1996,  5056,  1997,  4352,  1037,\n",
      "         11581, 18642,  1997,  2334,  2632, 28759,  1998, 12495, 18719, 24872,\n",
      "          2006,  4860,  1516,  1996, 10567,  2064,  2022,  3039,  2000,  2022,\n",
      "         13580,  1998,  1996, 26640,  4010,  1516,  2021,  1996,  3768,  1997,\n",
      "          2995, 10949,  2965,  2008,  9876, 19003,  2031,  2000,  2022,  9675,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  8040, 26837,  4779,  3678,  4858,  2659,  4785, 14639,  1012,\n",
      "           102,  4785, 14639,  2003,  1996,  6706,  2110,  2689,  1999,  1996,\n",
      "         14442,  4860,  2004,  1037,  2765,  1997,  3431,  1999,  1996,  2943,\n",
      "          5166,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "  attention_mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n",
      "  labels:  tensor([2, 1, 3, 2, 2, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class convertPytorchDataset(Dataset):\n",
    "    def __init__(self, claim_data, evidence_data, tokenizer, label_mapping):\n",
    "        self.claim_data = claim_data\n",
    "        self.evidence_data = evidence_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        claim_id, claim_info = list(self.claim_data.items())[idx]\n",
    "        claim_text = claim_info[\"claim_text\"]\n",
    "        evidence_texts = [self.evidence_data[evidence_id] for evidence_id in claim_info[\"evidences\"]]\n",
    "        all_evidence_text = ' '.join(evidence_texts)\n",
    "\n",
    "        # Tokenize the data, make them in the same length and not too long, and return as pytorch tensor\n",
    "        tokenized_data = self.tokenizer(claim_text, all_evidence_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Convert labels to numbers\n",
    "        tokenized_data[\"labels\"] = torch.tensor(self.label_mapping[claim_info[\"claim_label\"]])\n",
    "\n",
    "        return tokenized_data\n",
    "\n",
    "label_mapping = {\n",
    "    \"SUPPORTS\": 0,\n",
    "    \"REFUTES\": 1,\n",
    "    \"NOT_ENOUGH_INFO\": 2,\n",
    "    \"DISPUTED\": 3,\n",
    "}\n",
    "\n",
    "train_dataset = convertPytorchDataset(train_data, evidence_data, tokenizer, label_mapping)\n",
    "dev_dataset = convertPytorchDataset(dev_data, evidence_data, tokenizer, label_mapping)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = max([x[\"input_ids\"].shape[1] for x in batch])\n",
    "    input_ids = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    attention_mask = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    labels = torch.zeros(len(batch), dtype=torch.long)\n",
    "    \n",
    "    for i, x in enumerate(batch):\n",
    "        cur_len = x[\"input_ids\"].shape[1]\n",
    "        input_ids[i, :cur_len] = x[\"input_ids\"]\n",
    "        attention_mask[i, :cur_len] = x[\"attention_mask\"]\n",
    "        labels[i] = x[\"labels\"]\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|‚ñç         | 6/154 [00:26<10:51,  4.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     44\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     train_loss \u001b[39m=\u001b[39m train_epoch(model, train_dataloader, optimizer, scheduler, device)\n\u001b[1;32m     46\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 34\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39mattention_mask, labels\u001b[39m=\u001b[39mlabels)\n\u001b[1;32m     32\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m---> 34\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "num_labels = len(label_mapping)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# If gpu available, use gpu.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 3\n",
    "lr = 3e-5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            _, predictions = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            total_accuracy += accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            num_batches += 1\n",
    "\n",
    "    return total_accuracy / num_batches\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = evaluate(model, dev_dataloader, device)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
